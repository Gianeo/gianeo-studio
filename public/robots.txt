# Robots.txt for giannifavaretto.com
# Professional design leadership portfolio

User-agent: *
Allow: /

# Allow all major search engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

User-agent: YandexBot
Allow: /

# Block common spam/scraping bots
User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Allow social media crawlers for rich previews
User-agent: facebookexternalhit/*
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: WhatsApp
Allow: /

User-agent: TelegramBot
Allow: /

# Block file types that shouldn't be indexed
Disallow: *.pdf$
Disallow: *.doc$
Disallow: *.docx$
Disallow: *.xls$
Disallow: *.xlsx$
Disallow: *.ppt$
Disallow: *.pptx$

# Block development and system files
Disallow: /api/
Disallow: /_next/
Disallow: /.next/
Disallow: /node_modules/
Disallow: /.git/
Disallow: /.env
Disallow: /package.json
Disallow: /package-lock.json
Disallow: /yarn.lock
Disallow: /.gitignore
Disallow: /README.md

# Block common WordPress/CMS paths (if not using)
Disallow: /wp-admin/
Disallow: /wp-content/
Disallow: /wp-includes/
Disallow: /admin/
Disallow: /login/
Disallow: /dashboard/

# Allow important assets
Allow: /images/
Allow: /favicon.ico
Allow: /favicon.svg
Allow: /apple-touch-icon.png
Allow: /manifest.json
Allow: /browserconfig.xml

# Crawl-delay to be respectful of server resources
Crawl-delay: 1

# Sitemap location
Sitemap: https://giannifavaretto.com/sitemap.xml

# Host specification (helps with canonicalization)
Host: https://giannifavaretto.com